{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qpfnzh8VXrT"
      },
      "source": [
        "# **Modeling - Parisian Rents**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRMCPpmA08Fo"
      },
      "source": [
        "As we did the Exploratory Data Analysis in the previous notebook, let's work on the model that will try to predict the Paris rent of a location based on the features. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "71QsVo-YUrfT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "linreg = LinearRegression()\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "dv = DictVectorizer()\n",
        "from sklearn import linear_model\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "import xgboost as xgb\n",
        "\n",
        "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
        "from hyperopt.pyll import scope"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVgUPSev4hSn"
      },
      "source": [
        "## **Feature Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "o0LbHQlWi-Qj"
      },
      "outputs": [],
      "source": [
        "# Get the data\n",
        "df = pd.read_csv(\"data/logement-encadrement-des-loyers.csv\", sep=\";\")\n",
        "df_set = df.copy()[['Secteurs géographiques', \n",
        "                    'Numéro du quartier', \n",
        "                    'Nombre de pièces principales', \n",
        "                    'Epoque de construction',\n",
        "                    'Type de location', \n",
        "                    'Loyers de référence']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOgW2ypHPxyJ",
        "outputId": "ed02a126-b76d-4fef-bef9-65b1b14265e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Secteurs géographiques', 'Numéro du quartier', 'Nom du quartier',\n",
              "       'Nombre de pièces principales', 'Epoque de construction',\n",
              "       'Type de location', 'Loyers de référence',\n",
              "       'Loyers de référence majorés', 'Loyers de référence minorés', 'Année',\n",
              "       'Ville', 'Numéro INSEE du quartier'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYVN2mLelTqT"
      },
      "outputs": [],
      "source": [
        "# Preprocessing function\n",
        "def preprocess(df: pd.DataFrame, dv: DictVectorizer, fit_dv: bool = False):\n",
        "    ''' Create feature value vectors and one hot encode categorical\n",
        "        columns.\n",
        "\n",
        "        Args :\n",
        "          - df : A Dataframe\n",
        "          - dv : A DictVectorizer object (from Scikit Learn)\n",
        "          - fit_dv : True or False, if True, it fits the dv with \n",
        "                     the specified Dataframe.\n",
        "    '''\n",
        "    categorical = ['Epoque de construction', 'Type de location']\n",
        "    numerical = ['Secteurs géographiques', 'Numéro du quartier', \n",
        "                  'Nombre de pièces principales']\n",
        "    dicts = df[categorical + numerical].to_dict(orient='records')\n",
        "    if fit_dv:\n",
        "        X = dv.fit_transform(dicts)\n",
        "    else:\n",
        "        X = dv.transform(dicts)\n",
        "    return X, dv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69eoHK9VriMC"
      },
      "outputs": [],
      "source": [
        "# Split train and test datasets\n",
        "df_train, df_test = train_test_split(df_set, test_size=0.20, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa-6op8AmwiX"
      },
      "outputs": [],
      "source": [
        "# Preprocess the data, split Inputs and Outputs\n",
        "target = 'Loyers de référence'\n",
        "\n",
        "# Inputs and DictVectorizer\n",
        "X, dv = preprocess(df = df_train, dv = dv, fit_dv = True)\n",
        "# Ouputs\n",
        "y = df_train[target].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfJlq799nstv",
        "outputId": "a90c0270-af25-4995-93a8-bd2e8ec239ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Epoque de construction=1946-1970',\n",
              "       'Epoque de construction=1971-1990',\n",
              "       'Epoque de construction=Apres 1990',\n",
              "       'Epoque de construction=Avant 1946',\n",
              "       'Nombre de pièces principales', 'Numéro du quartier',\n",
              "       'Secteurs géographiques', 'Type de location=meublé',\n",
              "       'Type de location=non meublé'], dtype=object)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's see the features that the dv chose\n",
        "dv.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66m-5-dk7qQZ"
      },
      "outputs": [],
      "source": [
        "# Save the Dictvectorizer to use it in preprocessing during production\n",
        "# Save the model with pickle\n",
        "filename = 'preprocessor.b'\n",
        "pickle.dump(dv, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlepZJwFuH9N"
      },
      "source": [
        "##  **The Baseline : Linear Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2EZneTfpBis"
      },
      "source": [
        "I chose a simple Linear Regression to be my baseline. The idea will be to beat the score of the Linear Regression Model I will build. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bFzcfXojSUc",
        "outputId": "fad8baef-1b24-44bb-b0fa-87a87b42eb05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results of the CV : [0.8120212116199752, 0.807290992021981, 0.793182170945342, 0.804447990350114, 0.8113913808114921]\n"
          ]
        }
      ],
      "source": [
        "# A cross validation on the train dataset\n",
        "cv_results = cross_validate(linreg, X.toarray(), y)\n",
        "print(f\"Results of the CV : {[i for i in cv_results['test_score']]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DD22O6_s0A9",
        "outputId": "6904a736-16f5-4482-90f7-8c66035637ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fit the linear regression\n",
        "linreg.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pg6KLFynrcu2"
      },
      "outputs": [],
      "source": [
        "# Define our test dataset from the preprocess function\n",
        "target = 'Loyers de référence'\n",
        "\n",
        "X_test, _ = preprocess(df = df_test, dv = dv, fit_dv = False)\n",
        "y_test = df_test[target].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4CqXXDCsE54",
        "outputId": "d84f26dc-3c05-42fe-f7b7-fd9d0043c641"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8131377842672293"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the score on the test dataset\n",
        "linreg.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZWaszG1urHk",
        "outputId": "840313d2-6b07-478a-aaff-c4d792b4c5fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.7518591698739399"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get RMSE of our test data\n",
        "y_preds = linreg.predict(X_test)\n",
        "mean_squared_error(y_test, y_preds, squared=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvGvuwd8CdIK"
      },
      "source": [
        "We have now our **baseline**, the score that we have to beat is **81%** of accuracy and a RMSE of **1.736**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS2DMCrBuMfa"
      },
      "source": [
        "## **Multiple Algorithms Implementation (Lasso, RandomForest, SVC, XGboost)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17sFSk39pvrB"
      },
      "source": [
        "Now that we know the baseline to beat, let's create a cross validation on some algorithms and see which one performs the best. After that, we will have to find the best hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRp3sOCPvOHF"
      },
      "outputs": [],
      "source": [
        "# Compare each model with a Cross Validation of 3 splits\n",
        "def find_best_model_CV(X, y):\n",
        "  ''' Do a cross validation on a models dictionnary.\n",
        "\n",
        "      Args:\n",
        "        - X : inputs\n",
        "        - y : outputs (target predicted)\n",
        "      \n",
        "      Returns :\n",
        "        It returns the results of the CV : the RMSE and\n",
        "        the STD of the scoring value.\n",
        "  '''\n",
        "  \n",
        "  models = {'LinearRegression' : linear_model.Lasso(), 'RandomForestRegressor' : RandomForestRegressor(max_depth=2, random_state=0), \n",
        "            'PolynomialSVR' : SVR(kernel='poly'), 'XGBRegressor' : xgb.XGBRegressor(objective ='reg:squarederror')}\n",
        "  kfold = KFold(n_splits=3)\n",
        "  for model_name, model in models.items():\n",
        "    results2 = cross_validate(model, X, y, cv=kfold, scoring = 'neg_root_mean_squared_error')\n",
        "    print(f\"{model_name} - RMSE : {(results2['test_score'].mean())*-1} and STD {np.std(results2['test_score'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIrfYsXBwpU2",
        "outputId": "b652e599-22ce-435f-d8a5-fd8615d17a3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LinearRegression - RMSE : 2.8091079774318497 and STD 0.007165816625124331\n",
            "RandomForestRegressor - RMSE : 2.7766766358399195 and STD 0.03670564715077652\n",
            "PolynomialSVR - RMSE : 2.9380560541186127 and STD 0.009547957753102626\n",
            "XGBRegressor - RMSE : 0.9745685821364202 and STD 0.0036840826782667474\n"
          ]
        }
      ],
      "source": [
        "# Print each result for training data\n",
        "find_best_model_CV(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caTsvuwZ_Dr0"
      },
      "source": [
        "XGB Regressor is clearly the best model among those we have chosen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPoum_zx4yFK",
        "outputId": "0cd9cdb3-95c9-4214-a485-e4c0deab736e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9439643247485657"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Measure Generalization of the best model\n",
        "xgb_model = xgb.XGBRegressor(objective ='reg:squarederror')\n",
        "xgb_model.fit(X, y)\n",
        "\n",
        "xgb_model.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTiFTpuI_qIS",
        "outputId": "6c9ae022-6a6d-4fd7-814e-6024faf1c526"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9593360141649778"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get predictions and measure accuracy on true outputs\n",
        "y_preds = xgb_model.predict(X_test)\n",
        "mean_squared_error(y_test, y_preds, squared=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPVZ1dHa_WIK"
      },
      "source": [
        "It gets a great accuracy score and a RMSE under 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fsx6QJOqH93"
      },
      "source": [
        "## Hyperparameters Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW_Z3P7HqQa8"
      },
      "source": [
        "We have to find the best value of the hyperparameters of our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPK508i6ASr-",
        "outputId": "40690c03-4df9-4e23-860b-0d8467337974"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:12<00:00,  4.01it/s, best loss: 0.5148224655184348]\n",
            "{'eta': 0.4, 'max_depth': 11.0, 'min_samples_leaf': 1.0, 'min_samples_split': 3.0, 'n_estimators': 50.0}\n"
          ]
        }
      ],
      "source": [
        "# Define an objective function with a loss function\n",
        "# that hyperopt will minimize\n",
        "def objective(params):\n",
        "      ''' An objective function that declare a model\n",
        "          from a set of hyperparameters, fit it on \n",
        "          the training data and compute the rmse with\n",
        "          real outputs/predicted outputs.\n",
        "\n",
        "          Args:\n",
        "            - params : dict of hyperparameters\n",
        "          \n",
        "          Returns:\n",
        "            A dictionnary of some variables such as the\n",
        "            loss, the model...\n",
        "      '''\n",
        "      model = xgb.XGBRegressor(**params)\n",
        "      model.fit(X, y)\n",
        "      y_preds = model.predict(X_test)\n",
        "      rmse = mean_squared_error(y_test, y_preds, squared=False)\n",
        "\n",
        "      return {'loss': rmse, 'status': STATUS_OK, 'model': model}\n",
        "\n",
        "# A search space of hyperparameters that the \n",
        "# hyperopt min function will use to find the \n",
        "# best set of hyperparameters\n",
        "search_space = {\n",
        "    'objective' : 'reg:squarederror',\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 20, 1)),\n",
        "    'eta' : scope.int(hp.quniform('eta', 0, 1, 0.1)),\n",
        "    'n_estimators': scope.int(hp.quniform('n_estimators', 10, 50, 1)),\n",
        "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 4, 1)),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Initialize an empty trials database used to stored\n",
        "# data from the hyperopt process\n",
        "trials = Trials()\n",
        "\n",
        "# For reproducible results\n",
        "rstate = np.random.RandomState(42)\n",
        "\n",
        "# Get the best set of hyperparameters\n",
        "best = fmin(\n",
        "    fn=objective,\n",
        "    space=search_space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=50,\n",
        "    trials=trials,\n",
        "    rstate=rstate\n",
        ")\n",
        "\n",
        "# Save and reload results\n",
        "pickle.dump(trials, open(\"xgbregressor_trials.p\", \"wb\"))\n",
        "trials = pickle.load(open(\"xgbregressor_trials.p\", \"rb\"))\n",
        "\n",
        "print(best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9024vFQMdyS_"
      },
      "outputs": [],
      "source": [
        "# Save the best model from trials\n",
        "best_model = trials.best_trial['result']['model']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODVjR2nCmkiK",
        "outputId": "f0ec0e40-d342-4cfa-ce5d-af6d637bbeff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.983862431714724"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fit the best model with our training data\n",
        "best_model.fit(X, y)\n",
        "\n",
        "# Let's see the generalization score on our\n",
        "# test dataset\n",
        "best_model.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9JAnLI59YCs"
      },
      "source": [
        "The model is rendenring great on test data, that is a good indication about the generalization of it on new data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mF4aH7LbHEEv"
      },
      "outputs": [],
      "source": [
        "best_model.save_model(\"final_xgbregressor_model.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4W-HOtJepCru"
      },
      "outputs": [],
      "source": [
        "# Save the model with pickle\n",
        "filename = 'final_xgbregressor_model.pkl'\n",
        "pickle.dump(best_model, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DcKyfgk97OI"
      },
      "source": [
        "Let's deploy the model on a web application !"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Modeling Paris Rental.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.11 ('envN')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "9464dc767f7c60ae64a42c5c1b469992a2c5510f32b3f3e7e8f30262fd012fed"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
